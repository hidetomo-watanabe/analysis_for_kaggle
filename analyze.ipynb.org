{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import configparser\n",
    "import myfuncs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from subprocess import check_output\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Analyzer(object):\n",
    "    def __init__(self, config_path='./config.ini'):\n",
    "        self.cp = configparser.SafeConfigParser()\n",
    "        self.cp.read(config_path)\n",
    "        self.id_col = self.cp.get('data', 'id_col')\n",
    "        self.pred_col = self.cp.get('data', 'pred_col')\n",
    "\n",
    "    def get_base_model(self, modelname):\n",
    "        if modelname == 'log_reg':\n",
    "            return LogisticRegression()\n",
    "        elif modelname == 'svc':\n",
    "            return SVC()\n",
    "        elif modelname == 'l_svc':\n",
    "            return LinearSVC()\n",
    "        elif modelname == 'rf_clf':\n",
    "            return RandomForestClassifier()\n",
    "        elif modelname == 'gbdt_clf':\n",
    "            return GradientBoostingClassifier()\n",
    "        elif modelname == 'knn_clf':\n",
    "            return KNeighborsClassifier()\n",
    "        elif modelname == 'g_nb':\n",
    "            return GaussianNB()\n",
    "        elif modelname == 'preceptron':\n",
    "            return Perceptron()\n",
    "        elif modelname == 'sgd_clf':\n",
    "            return SGDClassifier()\n",
    "        elif modelname == 'dt_clf':\n",
    "            return DecisionTreeClassifier()\n",
    "\n",
    "    def display_raw_data(self):\n",
    "        for df in [self.train_df, self.test_df]:\n",
    "            display(df.head())\n",
    "            display(df.describe())\n",
    "\n",
    "    def replace_dfs(self, dfs, target, config, *, mean=None):\n",
    "        \"\"\"\n",
    "            config is dict\n",
    "        \"\"\"\n",
    "        output = []\n",
    "        for df in dfs:\n",
    "            for i, value1 in enumerate(df[target].values):\n",
    "                for key, value2 in config.items():\n",
    "                    # mean\n",
    "                    if value2 == 'mean':\n",
    "                        value2 = mean\n",
    "                    # translate\n",
    "                    if key == 'isnan':\n",
    "                        if math.isnan(value1):\n",
    "                            df[target].values[i] = value2\n",
    "                    else:\n",
    "                        if value1 == key:\n",
    "                            df[target].values[i] = value2\n",
    "            output.append(df)\n",
    "        return output\n",
    "\n",
    "    def categorize_dfs(self, dfs, target, config):\n",
    "        \"\"\"\n",
    "            config is list\n",
    "        \"\"\"\n",
    "        output = []\n",
    "        for df in dfs:\n",
    "            for value2 in config:\n",
    "                df['%s_%s' % (target, value2)] = [0] * len(df[target].values)\n",
    "            for i, value1 in enumerate(df[target].values):\n",
    "                for value2 in config:\n",
    "                    if value1 == value2:\n",
    "                        df['%s_%s' % (target, value2)].values[i] = 1\n",
    "            del df[target]\n",
    "            output.append(df)\n",
    "        return output\n",
    "\n",
    "    def to_float_dfs(self, dfs, pred_col, id_col):\n",
    "        output = []\n",
    "        for df in dfs:\n",
    "            for key in df.keys():\n",
    "                if key == pred_col or key == id_col:\n",
    "                    continue\n",
    "                df[key] = df[key].astype(float)\n",
    "            output.append(df)\n",
    "        return output\n",
    "\n",
    "    def get_raw_data(self):\n",
    "        print('### DATA LIST')\n",
    "        data_path = self.cp.get('data', 'path')\n",
    "        print(check_output(['ls', data_path]).decode('utf8'))\n",
    "        self.train_df = pd.read_csv('%s/train.csv' % data_path)\n",
    "        self.test_df = pd.read_csv('%s/test.csv' % data_path)\n",
    "        return self.train_df, self.test_df\n",
    "\n",
    "    def trans_raw_data(self):\n",
    "        train_df = self.train_df\n",
    "        test_df = self.test_df\n",
    "        cp = self.cp\n",
    "        trans_adhoc = json.loads(cp.get('translate', 'adhoc'))\n",
    "        trans_replace = json.loads(cp.get('translate', 'replace'))\n",
    "        trans_del = json.loads(cp.get('translate', 'del'))\n",
    "        trans_category = json.loads(cp.get('translate', 'category'))\n",
    "        # replace\n",
    "        for key, value in trans_replace.items():\n",
    "            # mean\n",
    "            if train_df.dtypes[key] == 'object':\n",
    "                key_mean = None\n",
    "            else:\n",
    "                key_mean = train_df[key].mean()\n",
    "            train_df, test_df = self.replace_dfs(\n",
    "                [train_df, test_df], key, value, mean=key_mean)\n",
    "        # adhoc\n",
    "        for value in trans_adhoc:\n",
    "            train_df, test_df = eval(\n",
    "                'myfuncs.%s' % value)([train_df, test_df], train_df)\n",
    "        # category\n",
    "        for key, values in trans_category.items():\n",
    "            train_df, test_df = self.categorize_dfs(\n",
    "                [train_df, test_df], key, values)\n",
    "        # del\n",
    "        for value in trans_del:\n",
    "            del train_df[value]\n",
    "            del test_df[value]\n",
    "        # del std=0\n",
    "        if self.cp.getboolean('translate', 'del_std0'):\n",
    "            for column in test_df.columns:\n",
    "                if np.std(train_df[column].values) == 0:\n",
    "                    if np.std(test_df[column].values) == 0:\n",
    "                        del train_df[column]\n",
    "                        del test_df[column]\n",
    "        # float\n",
    "        train_df, test_df = self.to_float_dfs(\n",
    "            [train_df, test_df], self.pred_col, self.id_col)\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        return self.train_df, self.test_df\n",
    "\n",
    "    def get_fitting_data(self):\n",
    "        train_df = self.train_df\n",
    "        test_df = self.test_df\n",
    "        # random\n",
    "        if self.cp.getboolean('data', 'random'):\n",
    "            train_df = train_df.iloc[np.random.permutation(len(train_df))]\n",
    "        # Y_train\n",
    "        self.Y_train = train_df[self.pred_col].values\n",
    "        # X_train\n",
    "        del train_df[self.pred_col]\n",
    "        if self.id_col in train_df.columns:\n",
    "            del train_df[self.id_col]\n",
    "        self.X_train = train_df.values\n",
    "        # X_test\n",
    "        if self.id_col in test_df.columns:\n",
    "            self.id_pred = test_df[self.id_col].values\n",
    "            del test_df[self.id_col]\n",
    "        else:\n",
    "            self.id_pred = test_df.index + 1\n",
    "        self.X_test = test_df.values\n",
    "        return self.X_train, self.Y_train, self.X_test\n",
    "\n",
    "    def normalize_fitting_data(self):\n",
    "        ss = StandardScaler()\n",
    "        ss.fit(self.X_train)\n",
    "        self.X_train = ss.transform(self.X_train)\n",
    "        self.X_test = ss.transform(self.X_test)\n",
    "        return self.X_train, self.X_test\n",
    "\n",
    "    def is_ok_with_adversarial_validation(self):\n",
    "        def _get_adversarial_preds(X_train, X_test, adversarial):\n",
    "            # create data\n",
    "            tmp_X_train = X_train[:len(X_test)]\n",
    "            X_adv = np.concatenate((tmp_X_train, X_test), axis=0)\n",
    "            target_adv = np.concatenate(\n",
    "                (np.zeros(len(X_test)), np.ones(len(X_test))), axis=0)\n",
    "            # fit\n",
    "            gs = GridSearchCV(\n",
    "                self.get_base_model(adversarial['model']),\n",
    "                adversarial['params'],\n",
    "                cv=adversarial['cv'],\n",
    "                scoring=adversarial['scoring'],\n",
    "                n_jobs=adversarial['n_jobs'])\n",
    "            gs.fit(X_adv, target_adv)\n",
    "            est = gs.best_estimator_\n",
    "            return est.predict(tmp_X_train), est.predict(X_test)\n",
    "\n",
    "        def _is_ok_pred_nums(tr0, tr1, te0, te1):\n",
    "            if tr0 == 0 and te0 == 0:\n",
    "                return False\n",
    "            if tr1 == 0 and te1 == 0:\n",
    "                return False\n",
    "            if tr1 == 0 and te0 == 0:\n",
    "                return False\n",
    "            return True\n",
    "\n",
    "        print('### DATA VALIDATION')\n",
    "        X_train = self.X_train\n",
    "        adversarial = self.cp.get('data', 'adversarial')\n",
    "        if adversarial:\n",
    "            print('with adversarial')\n",
    "            adversarial = json.loads(adversarial)\n",
    "            adv_pred_train, adv_pred_test = _get_adversarial_preds(\n",
    "                X_train, self.X_test, adversarial)\n",
    "            adv_pred_train_num_0 = len(np.where(adv_pred_train == 0)[0])\n",
    "            adv_pred_train_num_1 = len(np.where(adv_pred_train == 1)[0])\n",
    "            adv_pred_test_num_0 = len(np.where(adv_pred_test == 0)[0])\n",
    "            adv_pred_test_num_1 = len(np.where(adv_pred_test == 1)[0])\n",
    "            print('pred train num 0: %s' % adv_pred_train_num_0)\n",
    "            print('pred train num 1: %s' % adv_pred_train_num_1)\n",
    "            print('pred test num 0: %s' % adv_pred_test_num_0)\n",
    "            print('pred test num 1: %s' % adv_pred_test_num_1)\n",
    "            if not _is_ok_pred_nums(\n",
    "                adv_pred_train_num_0,\n",
    "                adv_pred_train_num_1,\n",
    "                adv_pred_test_num_0,\n",
    "                adv_pred_test_num_1,\n",
    "            ):\n",
    "                raise Exception(\n",
    "                    '[ERROR] TRAIN AND TEST MAY BE HAVE DIFFERENT FEATURES')\n",
    "        else:\n",
    "            print('[WARN] NO DATA VALIDATION')\n",
    "            return True\n",
    "\n",
    "    def calc_best_model(self):\n",
    "        print('### FIT')\n",
    "        base_model = self.get_base_model(self.cp.get('fit', 'model'))\n",
    "        scoring = self.cp.get('fit', 'scoring')\n",
    "        cv = self.cp.getint('fit', 'cv')\n",
    "        params = json.loads(self.cp.get('fit', 'params'))\n",
    "        gs = GridSearchCV(\n",
    "            base_model, params, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "        gs.fit(self.X_train, self.Y_train)\n",
    "        print('X train shape: %s' % str(self.X_train.shape))\n",
    "        print('Y train shape: %s' % str(self.Y_train.shape))\n",
    "        print('best params: %s' % gs.best_params_)\n",
    "        print('best score of trained grid search: %s' % gs.best_score_)\n",
    "        self.best_model = gs.best_estimator_\n",
    "        print('best model: %s' % self.best_model)\n",
    "        return self.best_model\n",
    "\n",
    "    def calc_output(self, filename):\n",
    "        Y_pred = self.best_model.predict(self.X_test)\n",
    "        f = open('outputs/%s' % filename, 'w')\n",
    "        f.write('%s,%s' % (self.id_col, self.pred_col))\n",
    "        for i in range(len(self.id_pred)):\n",
    "            f.write('\\n')\n",
    "            f.write('%s,%s' % (self.id_pred[i], Y_pred[i]))\n",
    "        f.close()\n",
    "        return filename\n",
    "\n",
    "    def visualize(self):\n",
    "        print('### SIMPLE VISUALIZATION')\n",
    "        for key in self.train_df.keys():\n",
    "            if key == self.pred_col or key == self.id_col:\n",
    "                continue\n",
    "            g = sns.FacetGrid(self.train_df, col=self.pred_col)\n",
    "            g.map(plt.hist, key, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1e1e8eda-928d-40e4-a38a-2d01c30d1fc1",
    "_execution_state": "idle",
    "_uuid": "eefd45171d68ec50244979f13ed5ba7225fb3243",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def translate_age(dfs, train_df):\n",
    "    #######################################\n",
    "    # no age => mean grouped Mr, Mrs, Miss\n",
    "    #######################################\n",
    "    # get honorific title\n",
    "    train_df['HonorificTitle'] = [''] * len(train_df['Name'].values)\n",
    "    for i, val in enumerate(train_df['Name'].values):\n",
    "        train_df['HonorificTitle'].values[i] = val.split(',')[1].split('.')[0]\n",
    "    # get honorific title => age mean\n",
    "    t2m = {}\n",
    "    tmp = train_df.groupby('HonorificTitle')['Age']\n",
    "    for key in tmp.indices.keys():\n",
    "        t2m[key] = tmp.mean()[key]\n",
    "    # age range\n",
    "    for df in dfs:\n",
    "        for i, val in enumerate(df['Age'].values):\n",
    "            if math.isnan(val):\n",
    "                val = t2m[df['Name'].values[i].split(',')[1].split('.')[0]]\n",
    "                df['Age'].values[i] = val\n",
    "    # del honorific title\n",
    "    del train_df['HonorificTitle']\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def translate_familystatus(dfs, train_df):\n",
    "    #######################################\n",
    "    # sibsp + parch == 0 => no family(0)\n",
    "    # survive vs no survive in same familyname\n",
    "    # => more survive(1) or less survive(2)\n",
    "    # categorize after\n",
    "    #######################################\n",
    "    # get family name\n",
    "    train_df['FamilyName'] = [''] * len(train_df['Name'].values)\n",
    "    for i, val in enumerate(train_df['Name'].values):\n",
    "        train_df['FamilyName'].values[i] = val.split(',')[0]\n",
    "    # get family name => family status\n",
    "    n2s = {}\n",
    "    tmp = train_df.groupby('FamilyName')['Survived']\n",
    "    for key in tmp.indices.keys():\n",
    "        survived_num = tmp.sum()[key]\n",
    "        no_survived_num = tmp.count()[key] - survived_num\n",
    "        if survived_num > no_survived_num:\n",
    "            n2s[key] = 1\n",
    "        else:\n",
    "            n2s[key] = 2\n",
    "    # main\n",
    "    for df in dfs:\n",
    "        df['FamilyStatus'] = [0] * len(df['Name'].values)\n",
    "        for i, val in enumerate(df['Name'].values):\n",
    "            family_name = val.split(',')[0]\n",
    "            # no family\n",
    "            if df['SibSp'].values[i] + df['Parch'].values[i] == 0:\n",
    "                continue\n",
    "            # any family\n",
    "            if family_name in n2s:\n",
    "                df['FamilyStatus'].values[i] = n2s[family_name]\n",
    "    # del family name\n",
    "    del train_df['FamilyName']\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2c80bcc2-ebcf-4c43-bdf0-6c4299566a56",
    "_execution_state": "idle",
    "_uuid": "9b60b1203d2797ec68812ccf7c02c9e31bff718b",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# definition\n",
    "# for jupyter\n",
    "config_text = \"\"\"\n",
    "[data]\n",
    "train_path = ../input/titanic/train.csv\n",
    "test_path = ../input/titanic/test.csv\n",
    "pred_col = Hoge\n",
    "id_col = Id\n",
    "random = False\n",
    "adversarial = {\n",
    "    \"model\": \"log_reg\",\n",
    "    \"scoring\": \"accuracy\",\n",
    "    \"cv\": 3,\n",
    "    \"n_jobs\": -1,\n",
    "    \"params\" : {\n",
    "    \"penalty\": [\"l1\", \"l2\"],\n",
    "    \"C\": [1, 2, 3, 4, 5]\n",
    "        }\n",
    "    }\n",
    "\n",
    "[fit]\n",
    "model = gbdt_clf\n",
    "scoring = accuracy\n",
    "cv = 3\n",
    "n_jobs = -1\n",
    "params = {\n",
    "    \"n_estimators\": [1, 2, 3, 4, 5],\n",
    "    \"max_depth\": [1, 2, 3, 4, 5]\n",
    "    }\n",
    "\n",
    "[translate]\n",
    "replace = {\n",
    "    \"Hoge\": {\"hoge1\": 1, \"hoge2\": -1},\n",
    "    \"Bar\": {\"isnan\": \"mean\"}\n",
    "    }\n",
    "adhoc = [\n",
    "    \"hoge.translate_hoge\",\n",
    "    \"hoge.translate_fuga\"\n",
    "    ]\n",
    "category = {\n",
    "    \"Hoge\": [1, 2, 3],\n",
    "    \"Fuga\": [\"a\", \"b\", \"c\"]\n",
    "    }\n",
    "del = [\n",
    "    \"Hoge\",\n",
    "    \"Foo\"\n",
    "    ]\n",
    "del_std0 = False\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c8e70134-2005-4d80-984a-37fa7604a47e",
    "_execution_state": "idle",
    "_uuid": "d37904a3ec4a18a302373a89e220d93c03fb8c0c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Analyzer import Analyzer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "analyzer_obj = Analyzer()\n",
    "analyzer_obj.read_config_text(config_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "analyzer_obj.get_raw_data()\n",
    "print('### INIT OVERVIEW')\n",
    "analyzer_obj.display_raw_data()\n",
    "analyzer_obj.trans_raw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('### TRANSLATION OVERVIEW')\n",
    "analyzer_obj.display_raw_data()\n",
    "analyzer_obj.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "analyzer_obj.get_fitting_data()\n",
    "analyzer_obj.normalize_fitting_data()\n",
    "analyzer_obj.is_ok_with_adversarial_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "analyzer_obj.calc_best_model('tmp.pickle')\n",
    "analyzer_obj.calc_output('tmp.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
